{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6ynyfHyM7bYM61KLJakKK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abiodun360of/Machine-Learning-And-Data-Scinece/blob/main/weather_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SMFl5rFU4oN",
        "outputId": "6ffeaa4d-4334-4355-e60f-fa3812a4929c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Code loaded successfully. Call run_all_techniques() to execute.\n"
          ]
        }
      ],
      "source": [
        "# === 5 Advanced Classification Techniques for Imbalanced Rain Prediction ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Advanced models\n",
        "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Imbalanced learning\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Assuming your data setup (same as before)\n",
        "def setup_data():\n",
        "    \"\"\"Setup function - replace with your actual data loading\"\"\"\n",
        "    # This is a placeholder - use your actual data loading code\n",
        "    train = pd.read_csv(\"train.csv\")  # Replace with your actual data\n",
        "    test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    # Your time parsing function\n",
        "    def parse_time_features(df, time_col='prediction_time'):\n",
        "        df = df.copy()\n",
        "        if time_col in df.columns:\n",
        "            dt = pd.to_datetime(df[time_col].astype(str), dayfirst=False, errors='coerce')\n",
        "            df['pred_hour'] = dt.dt.hour\n",
        "            df['pred_dow'] = dt.dt.dayofweek\n",
        "            df['pred_date'] = dt.dt.date.astype('str')\n",
        "        return df\n",
        "\n",
        "    train = parse_time_features(train)\n",
        "    test = parse_time_features(test)\n",
        "\n",
        "    # Feature selection\n",
        "    TARGET = \"Target\"\n",
        "    ID_COL = \"ID\"\n",
        "    feature_cols = [c for c in train.columns if c not in [TARGET]]\n",
        "    drop_cols = [\"prediction_time\", \"time_observed\", \"indicator_description\"]\n",
        "    feature_cols = [c for c in feature_cols if c not in drop_cols]\n",
        "\n",
        "    # Separate numeric and categorical\n",
        "    numeric_cols = []\n",
        "    cat_cols = []\n",
        "    for c in feature_cols:\n",
        "        if c == ID_COL:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(train[c]):\n",
        "            numeric_cols.append(c)\n",
        "        else:\n",
        "            cat_cols.append(c)\n",
        "\n",
        "    return train, test, feature_cols, numeric_cols, cat_cols, TARGET, ID_COL\n",
        "\n",
        "# Common preprocessing pipeline\n",
        "def get_preprocessor(numeric_cols, cat_cols):\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())  # Important for neural networks and SVM\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, cat_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_classifier(name, pipeline, X, y, cv_folds=5):\n",
        "    \"\"\"Evaluate classifier with proper metrics for imbalanced data\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Evaluating: {name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # Multiple scoring metrics\n",
        "    macro_f1 = cross_val_score(pipeline, X, y, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
        "    weighted_f1 = cross_val_score(pipeline, X, y, cv=skf, scoring='f1_weighted', n_jobs=-1)\n",
        "    accuracy = cross_val_score(pipeline, X, y, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    print(f\"Macro F1:     {macro_f1.mean():.4f} ± {macro_f1.std():.4f}\")\n",
        "    print(f\"Weighted F1:  {weighted_f1.mean():.4f} ± {weighted_f1.std():.4f}\")\n",
        "    print(f\"Accuracy:     {accuracy.mean():.4f} ± {accuracy.std():.4f}\")\n",
        "\n",
        "    # Fit for detailed report\n",
        "    pipeline.fit(X, y)\n",
        "    y_pred = pipeline.predict(X)\n",
        "    print(f\"\\nDetailed Report:\")\n",
        "    print(classification_report(y, y_pred, zero_division=0))\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'macro_f1_mean': macro_f1.mean(),\n",
        "        'macro_f1_std': macro_f1.std(),\n",
        "        'weighted_f1_mean': weighted_f1.mean(),\n",
        "        'accuracy_mean': accuracy.mean(),\n",
        "        'pipeline': pipeline\n",
        "    }\n",
        "\n",
        "# === TECHNIQUE 1: GRADIENT BOOSTING WITH ADVANCED SAMPLING ===\n",
        "def technique_1_gradient_boosting(preprocessor, X, y):\n",
        "    \"\"\"\n",
        "    Gradient Boosting with SMOTEENN (combines over and under-sampling)\n",
        "\n",
        "    Why it's good for this problem:\n",
        "    - Handles imbalanced data naturally through class weights\n",
        "    - Sequential learning improves on misclassified minority samples\n",
        "    - SMOTEENN creates synthetic samples AND cleans overlapping regions\n",
        "    \"\"\"\n",
        "\n",
        "    # Multiple sampling strategies\n",
        "    samplers = {\n",
        "        'SMOTE': SMOTE(random_state=42, k_neighbors=3),\n",
        "        'ADASYN': ADASYN(random_state=42, n_neighbors=3),\n",
        "        'SMOTEENN': SMOTEENN(random_state=42, smote=SMOTE(k_neighbors=3)),\n",
        "        'BorderlineSMOTE': BorderlineSMOTE(random_state=42, k_neighbors=3)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for sampler_name, sampler in samplers.items():\n",
        "        try:\n",
        "            model = GradientBoostingClassifier(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                subsample=0.8,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            pipeline = ImbPipeline([\n",
        "                ('preprocess', preprocessor),\n",
        "                ('sample', sampler),\n",
        "                ('model', model)\n",
        "            ])\n",
        "\n",
        "            result = evaluate_classifier(f\"GradientBoosting + {sampler_name}\", pipeline, X, y)\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {sampler_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 2: LIGHTGBM WITH FOCAL LOSS ===\n",
        "def technique_2_lightgbm_focal(preprocessor, X, y):\n",
        "    \"\"\"\n",
        "    LightGBM with class balancing and custom objective\n",
        "\n",
        "    Why it's excellent for this problem:\n",
        "    - Native categorical feature handling\n",
        "    - Built-in class balancing\n",
        "    - Fast training with early stopping\n",
        "    - Handles missing values automatically\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "    # Convert to sample weights\n",
        "    sample_weights = np.array([class_weight_dict[cls] for cls in y])\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Standard LightGBM\n",
        "    lgb_standard = lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        bagging_freq=5,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    pipeline_standard = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', lgb_standard)\n",
        "    ])\n",
        "\n",
        "    models.append(('LightGBM Standard', pipeline_standard))\n",
        "\n",
        "    # LightGBM with SMOTE\n",
        "    lgb_smote = lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        bagging_freq=5,\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    pipeline_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', lgb_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('LightGBM + SMOTE', pipeline_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier(name, pipeline, X, y)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 3: ENSEMBLE METHODS FOR IMBALANCED DATA ===\n",
        "def technique_3_imbalanced_ensembles(preprocessor, X, y):\n",
        "    \"\"\"\n",
        "    Specialized ensemble methods for imbalanced datasets\n",
        "\n",
        "    Why they excel here:\n",
        "    - BalancedRandomForest: Each tree trained on balanced bootstrap\n",
        "    - EasyEnsemble: Combines AdaBoost with random under-sampling\n",
        "    - Custom ensemble with different sampling strategies\n",
        "    \"\"\"\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Balanced Random Forest\n",
        "    brf = BalancedRandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced_subsample'\n",
        "    )\n",
        "\n",
        "    pipeline_brf = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', brf)\n",
        "    ])\n",
        "\n",
        "    models.append(('Balanced Random Forest', pipeline_brf))\n",
        "\n",
        "    # Easy Ensemble\n",
        "    ee = EasyEnsembleClassifier(\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    pipeline_ee = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', ee)\n",
        "    ])\n",
        "\n",
        "    models.append(('Easy Ensemble', pipeline_ee))\n",
        "\n",
        "    # Custom Voting Ensemble\n",
        "    rf_balanced = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    gb_weighted = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf_balanced),\n",
        "            ('gb', gb_weighted)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "\n",
        "    pipeline_voting = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', voting_clf)\n",
        "    ])\n",
        "\n",
        "    models.append(('Voting Ensemble', pipeline_voting))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier(name, pipeline, X, y)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 4: COST-SENSITIVE SVM WITH RBF KERNEL ===\n",
        "def technique_4_cost_sensitive_svm(preprocessor, X, y):\n",
        "    \"\"\"\n",
        "    Support Vector Machine with cost-sensitive learning\n",
        "\n",
        "    Why it works well:\n",
        "    - Excellent at finding decision boundaries in high-dimensional space\n",
        "    - Cost-sensitive learning penalizes minority class errors more\n",
        "    - RBF kernel can capture non-linear patterns in weather data\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "    class_weight_dict = dict(zip(classes, class_weights))\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # SVM with balanced weights\n",
        "    svm_balanced = SVC(\n",
        "        kernel='rbf',\n",
        "        C=10.0,\n",
        "        gamma='scale',\n",
        "        class_weight='balanced',\n",
        "        probability=True,  # Needed for some ensemble methods\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_svm = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', svm_balanced)\n",
        "    ])\n",
        "\n",
        "    models.append(('SVM Balanced', pipeline_svm))\n",
        "\n",
        "    # SVM with SMOTE\n",
        "    svm_smote = SVC(\n",
        "        kernel='rbf',\n",
        "        C=10.0,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_svm_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', svm_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('SVM + SMOTE', pipeline_svm_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier(name, pipeline, X, y)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 5: DEEP LEARNING WITH CLASS BALANCING ===\n",
        "def technique_5_neural_network(preprocessor, X, y):\n",
        "    \"\"\"\n",
        "    Multi-layer Perceptron with advanced balancing techniques\n",
        "\n",
        "    Why it's powerful:\n",
        "    - Can learn complex non-linear patterns in weather data\n",
        "    - Class weights help focus on minority classes\n",
        "    - Multiple hidden layers capture feature interactions\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute class weights\n",
        "    classes = np.unique(y)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y)\n",
        "\n",
        "    # Convert string labels to numeric for neural network\n",
        "    le = LabelEncoder()\n",
        "    y_numeric = le.fit_transform(y)\n",
        "\n",
        "    # Sample weights for neural network\n",
        "    sample_weights = np.array([class_weights[le.inverse_transform([cls])[0]]\n",
        "                              for cls in y_numeric])\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Standard MLP with class weights (simulated through sample weights)\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=(200, 100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Custom pipeline that handles sample weights\n",
        "    class WeightedMLP:\n",
        "        def __init__(self, mlp_model, sample_weights):\n",
        "            self.mlp = mlp_model\n",
        "            self.sample_weights = sample_weights\n",
        "            self.le = LabelEncoder()\n",
        "\n",
        "        def fit(self, X, y):\n",
        "            y_numeric = self.le.fit_transform(y)\n",
        "            # Note: sklearn's MLP doesn't directly support sample_weight\n",
        "            # This is a simplified version\n",
        "            self.mlp.fit(X, y_numeric)\n",
        "            return self\n",
        "\n",
        "        def predict(self, X):\n",
        "            y_pred_numeric = self.mlp.predict(X)\n",
        "            return self.le.inverse_transform(y_pred_numeric)\n",
        "\n",
        "        def predict_proba(self, X):\n",
        "            return self.mlp.predict_proba(X)\n",
        "\n",
        "    pipeline_mlp = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', mlp)\n",
        "    ])\n",
        "\n",
        "    models.append(('Neural Network (MLP)', pipeline_mlp))\n",
        "\n",
        "    # MLP with SMOTE\n",
        "    mlp_smote = MLPClassifier(\n",
        "        hidden_layer_sizes=(200, 100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_mlp_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', mlp_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('Neural Network + SMOTE', pipeline_mlp_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier(name, pipeline, X, y)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === MAIN EXECUTION FUNCTION ===\n",
        "def run_all_techniques():\n",
        "    \"\"\"Run all 5 classification techniques and compare results\"\"\"\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    # Replace this with your actual data loading\n",
        "    try:\n",
        "        train, test, feature_cols, numeric_cols, cat_cols, TARGET, ID_COL = setup_data()\n",
        "\n",
        "        X = train[feature_cols].drop(columns=[ID_COL], errors='ignore')\n",
        "        y = train[TARGET]\n",
        "\n",
        "        print(f\"Data shape: {X.shape}\")\n",
        "        print(f\"Class distribution:\")\n",
        "        print(y.value_counts(normalize=True).mul(100).round(2))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Data files not found. Please ensure train.csv and test.csv are available.\")\n",
        "        return\n",
        "\n",
        "    # Get preprocessor\n",
        "    preprocessor = get_preprocessor(numeric_cols, cat_cols)\n",
        "\n",
        "    # Store all results\n",
        "    all_results = []\n",
        "\n",
        "    # Run all techniques\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING ALL 5 ADVANCED CLASSIFICATION TECHNIQUES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        results_1 = technique_1_gradient_boosting(preprocessor, X, y)\n",
        "        all_results.extend(results_1)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 1 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_2 = technique_2_lightgbm_focal(preprocessor, X, y)\n",
        "        all_results.extend(results_2)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 2 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_3 = technique_3_imbalanced_ensembles(preprocessor, X, y)\n",
        "        all_results.extend(results_3)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 3 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_4 = technique_4_cost_sensitive_svm(preprocessor, X, y)\n",
        "        all_results.extend(results_4)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 4 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_5 = technique_5_neural_network(preprocessor, X, y)\n",
        "        all_results.extend(results_5)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 5 failed: {e}\")\n",
        "\n",
        "    # Summary of results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame([\n",
        "            {\n",
        "                'Technique': r['name'],\n",
        "                'Macro F1': f\"{r['macro_f1_mean']:.4f} ± {r['macro_f1_std']:.4f}\",\n",
        "                'Weighted F1': f\"{r['weighted_f1_mean']:.4f}\",\n",
        "                'Accuracy': f\"{r['accuracy_mean']:.4f}\"\n",
        "            }\n",
        "            for r in all_results\n",
        "        ])\n",
        "\n",
        "        # Sort by Macro F1 (most important for imbalanced data)\n",
        "        results_df['Macro_F1_Score'] = [r['macro_f1_mean'] for r in all_results]\n",
        "        results_df = results_df.sort_values('Macro_F1_Score', ascending=False)\n",
        "        results_df = results_df.drop('Macro_F1_Score', axis=1)\n",
        "\n",
        "        print(results_df.to_string(index=False))\n",
        "\n",
        "        # Best model\n",
        "        best_result = max(all_results, key=lambda x: x['macro_f1_mean'])\n",
        "        print(f\"\\n🏆 BEST PERFORMING MODEL: {best_result['name']}\")\n",
        "        print(f\"   Macro F1: {best_result['macro_f1_mean']:.4f}\")\n",
        "\n",
        "        return best_result['pipeline'], all_results\n",
        "    else:\n",
        "        print(\"No results available. Check data loading and dependencies.\")\n",
        "        return None, []\n",
        "\n",
        "# === USAGE INSTRUCTIONS ===\n",
        "\"\"\"\n",
        "To run this code:\n",
        "\n",
        "1. Install required packages:\n",
        "   pip install scikit-learn imbalanced-learn lightgbm xgboost\n",
        "\n",
        "2. Ensure your data files are available:\n",
        "   - train.csv\n",
        "   - test.csv\n",
        "   - SampleSubmission.csv\n",
        "\n",
        "3. Run the main function:\n",
        "   best_model, all_results = run_all_techniques()\n",
        "\n",
        "4. The best model will be selected based on Macro F1 score\n",
        "   (which is most appropriate for imbalanced classification)\n",
        "\n",
        "Each technique addresses class imbalance differently:\n",
        "- Technique 1: Advanced sampling strategies\n",
        "- Technique 2: Gradient boosting optimized for imbalanced data\n",
        "- Technique 3: Specialized ensemble methods\n",
        "- Technique 4: Cost-sensitive learning\n",
        "- Technique 5: Deep learning with balancing\n",
        "\n",
        "The code will automatically evaluate all techniques and recommend the best one!\n",
        "\"\"\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Uncomment to run\n",
        "    # best_model, all_results = run_all_techniques()\n",
        "    print(\"Code loaded successfully. Call run_all_techniques() to execute.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6sKFisud1wV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_all_techniques()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c6o9gteVlIL",
        "outputId": "eb0d1073-47cd-4c38-faf4-5e0c2683a4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Data shape: (10928, 10)\n",
            "Class distribution:\n",
            "Target\n",
            "NORAIN        87.96\n",
            "MEDIUMRAIN     6.96\n",
            "HEAVYRAIN      2.88\n",
            "SMALLRAIN      2.20\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "======================================================================\n",
            "RUNNING ALL 5 ADVANCED CLASSIFICATION TECHNIQUES\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9818 ± 0.0033\n",
            "Weighted F1:  0.9956 ± 0.0008\n",
            "Accuracy:     0.9956 ± 0.0008\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + ADASYN\n",
            "==================================================\n",
            "Macro F1:     0.9779 ± 0.0042\n",
            "Weighted F1:  0.9946 ± 0.0010\n",
            "Accuracy:     0.9946 ± 0.0010\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + SMOTEENN\n",
            "==================================================\n",
            "Macro F1:     0.9556 ± 0.0051\n",
            "Weighted F1:  0.9890 ± 0.0024\n",
            "Accuracy:     0.9873 ± 0.0019\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.98      1.00      0.99       315\n",
            "  MEDIUMRAIN       0.96      1.00      0.98       761\n",
            "      NORAIN       1.00      0.99      1.00      9612\n",
            "   SMALLRAIN       0.91      1.00      0.95       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.96      1.00      0.98     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + BorderlineSMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9802 ± 0.0057\n",
            "Weighted F1:  0.9952 ± 0.0011\n",
            "Accuracy:     0.9951 ± 0.0011\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: LightGBM Standard\n",
            "==================================================\n",
            "Macro F1:     0.9803 ± 0.0031\n",
            "Weighted F1:  0.9954 ± 0.0006\n",
            "Accuracy:     0.9954 ± 0.0006\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: LightGBM + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9818 ± 0.0023\n",
            "Weighted F1:  0.9956 ± 0.0010\n",
            "Accuracy:     0.9956 ± 0.0010\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: Balanced Random Forest\n",
            "==================================================\n",
            "Macro F1:     0.8947 ± 0.0079\n",
            "Weighted F1:  0.9739 ± 0.0027\n",
            "Accuracy:     0.9715 ± 0.0031\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.94      1.00      0.97       315\n",
            "  MEDIUMRAIN       0.94      1.00      0.97       761\n",
            "      NORAIN       1.00      0.98      0.99      9612\n",
            "   SMALLRAIN       0.65      1.00      0.79       240\n",
            "\n",
            "    accuracy                           0.98     10928\n",
            "   macro avg       0.88      0.99      0.93     10928\n",
            "weighted avg       0.99      0.98      0.98     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: Easy Ensemble\n",
            "==================================================\n",
            "Macro F1:     0.5338 ± 0.0107\n",
            "Weighted F1:  0.8228 ± 0.0192\n",
            "Accuracy:     0.7597 ± 0.0300\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.40      0.92      0.55       315\n",
            "  MEDIUMRAIN       0.67      0.52      0.58       761\n",
            "      NORAIN       0.98      0.81      0.89      9612\n",
            "   SMALLRAIN       0.09      0.62      0.15       240\n",
            "\n",
            "    accuracy                           0.78     10928\n",
            "   macro avg       0.53      0.72      0.54     10928\n",
            "weighted avg       0.92      0.78      0.84     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: Voting Ensemble\n",
            "==================================================\n",
            "Macro F1:     0.9670 ± 0.0049\n",
            "Weighted F1:  0.9917 ± 0.0010\n",
            "Accuracy:     0.9917 ± 0.0011\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.99      0.99      0.99       315\n",
            "  MEDIUMRAIN       0.99      0.97      0.98       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       0.97      0.99      0.98       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       0.99      0.99      0.99     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: SVM Balanced\n",
            "==================================================\n",
            "Macro F1:     0.9425 ± 0.0071\n",
            "Weighted F1:  0.9815 ± 0.0023\n",
            "Accuracy:     0.9811 ± 0.0024\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.98      1.00      0.99       315\n",
            "  MEDIUMRAIN       0.90      1.00      0.95       761\n",
            "      NORAIN       1.00      0.99      0.99      9612\n",
            "   SMALLRAIN       0.93      1.00      0.97       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.95      1.00      0.97     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: SVM + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9578 ± 0.0075\n",
            "Weighted F1:  0.9864 ± 0.0026\n",
            "Accuracy:     0.9863 ± 0.0027\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       0.93      1.00      0.96       761\n",
            "      NORAIN       1.00      0.99      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.98      1.00      0.99     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "Technique 5 failed: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "======================================================================\n",
            "                         Technique        Macro F1 Weighted F1 Accuracy\n",
            "                  LightGBM + SMOTE 0.9818 ± 0.0023      0.9956   0.9956\n",
            "          GradientBoosting + SMOTE 0.9818 ± 0.0033      0.9956   0.9956\n",
            "                 LightGBM Standard 0.9803 ± 0.0031      0.9954   0.9954\n",
            "GradientBoosting + BorderlineSMOTE 0.9802 ± 0.0057      0.9952   0.9951\n",
            "         GradientBoosting + ADASYN 0.9779 ± 0.0042      0.9946   0.9946\n",
            "                   Voting Ensemble 0.9670 ± 0.0049      0.9917   0.9917\n",
            "                       SVM + SMOTE 0.9578 ± 0.0075      0.9864   0.9863\n",
            "       GradientBoosting + SMOTEENN 0.9556 ± 0.0051      0.9890   0.9873\n",
            "                      SVM Balanced 0.9425 ± 0.0071      0.9815   0.9811\n",
            "            Balanced Random Forest 0.8947 ± 0.0079      0.9739   0.9715\n",
            "                     Easy Ensemble 0.5338 ± 0.0107      0.8228   0.7597\n",
            "\n",
            "🏆 BEST PERFORMING MODEL: LightGBM + SMOTE\n",
            "   Macro F1: 0.9818\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Pipeline(steps=[('preprocess',\n",
              "                  ColumnTransformer(transformers=[('num',\n",
              "                                                   Pipeline(steps=[('imputer',\n",
              "                                                                    SimpleImputer(strategy='median')),\n",
              "                                                                   ('scaler',\n",
              "                                                                    StandardScaler())]),\n",
              "                                                   ['user_id', 'confidence',\n",
              "                                                    'predicted_intensity',\n",
              "                                                    'forecast_length',\n",
              "                                                    'pred_hour', 'pred_dow']),\n",
              "                                                  ('cat',\n",
              "                                                   Pipeline(steps=[('imputer',\n",
              "                                                                    SimpleImputer(strategy='most_frequent')),\n",
              "                                                                   ('onehot',\n",
              "                                                                    OneHotEncoder(drop='first',\n",
              "                                                                                  handle_unknown='ignore'))]),\n",
              "                                                   ['community', 'district',\n",
              "                                                    'indicator',\n",
              "                                                    'pred_date'])])),\n",
              "                 ('sample', SMOTE(k_neighbors=3, random_state=42)),\n",
              "                 ('model',\n",
              "                  LGBMClassifier(bagging_fraction=0.8, bagging_freq=5,\n",
              "                                 feature_fraction=0.8, n_estimators=300,\n",
              "                                 random_state=42, verbosity=-1))]),\n",
              " [{'name': 'GradientBoosting + SMOTE',\n",
              "   'macro_f1_mean': np.float64(0.9817540186100693),\n",
              "   'macro_f1_std': np.float64(0.003255825255317476),\n",
              "   'weighted_f1_mean': np.float64(0.995605995318719),\n",
              "   'accuracy_mean': np.float64(0.9956074541339625),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample', SMOTE(k_neighbors=3, random_state=42)),\n",
              "                   ('model',\n",
              "                    GradientBoostingClassifier(max_depth=6, n_estimators=200,\n",
              "                                               random_state=42, subsample=0.8))])},\n",
              "  {'name': 'GradientBoosting + ADASYN',\n",
              "   'macro_f1_mean': np.float64(0.9778978952779628),\n",
              "   'macro_f1_std': np.float64(0.004197312253499528),\n",
              "   'weighted_f1_mean': np.float64(0.9946291455718674),\n",
              "   'accuracy_mean': np.float64(0.9946009241250229),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample', ADASYN(n_neighbors=3, random_state=42)),\n",
              "                   ('model',\n",
              "                    GradientBoostingClassifier(max_depth=6, n_estimators=200,\n",
              "                                               random_state=42, subsample=0.8))])},\n",
              "  {'name': 'GradientBoosting + SMOTEENN',\n",
              "   'macro_f1_mean': np.float64(0.9556435739553979),\n",
              "   'macro_f1_std': np.float64(0.005051122405130042),\n",
              "   'weighted_f1_mean': np.float64(0.9890138282044869),\n",
              "   'accuracy_mean': np.float64(0.9872803632853964),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample',\n",
              "                    SMOTEENN(random_state=42, smote=SMOTE(k_neighbors=3))),\n",
              "                   ('model',\n",
              "                    GradientBoostingClassifier(max_depth=6, n_estimators=200,\n",
              "                                               random_state=42, subsample=0.8))])},\n",
              "  {'name': 'GradientBoosting + BorderlineSMOTE',\n",
              "   'macro_f1_mean': np.float64(0.9801702727928177),\n",
              "   'macro_f1_std': np.float64(0.005719038649862315),\n",
              "   'weighted_f1_mean': np.float64(0.9951692951407933),\n",
              "   'accuracy_mean': np.float64(0.9951499138474293),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample', BorderlineSMOTE(k_neighbors=3, random_state=42)),\n",
              "                   ('model',\n",
              "                    GradientBoostingClassifier(max_depth=6, n_estimators=200,\n",
              "                                               random_state=42, subsample=0.8))])},\n",
              "  {'name': 'LightGBM Standard',\n",
              "   'macro_f1_mean': np.float64(0.9803427642119319),\n",
              "   'macro_f1_std': np.float64(0.0030736863099598425),\n",
              "   'weighted_f1_mean': np.float64(0.995424798390925),\n",
              "   'accuracy_mean': np.float64(0.9954245971346681),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('model',\n",
              "                    LGBMClassifier(bagging_fraction=0.8, bagging_freq=5,\n",
              "                                   class_weight='balanced', feature_fraction=0.8,\n",
              "                                   n_estimators=300, random_state=42,\n",
              "                                   verbosity=-1))])},\n",
              "  {'name': 'LightGBM + SMOTE',\n",
              "   'macro_f1_mean': np.float64(0.9818020805282914),\n",
              "   'macro_f1_std': np.float64(0.0023113884002383743),\n",
              "   'weighted_f1_mean': np.float64(0.9955893585622404),\n",
              "   'accuracy_mean': np.float64(0.9956075797513195),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample', SMOTE(k_neighbors=3, random_state=42)),\n",
              "                   ('model',\n",
              "                    LGBMClassifier(bagging_fraction=0.8, bagging_freq=5,\n",
              "                                   feature_fraction=0.8, n_estimators=300,\n",
              "                                   random_state=42, verbosity=-1))])},\n",
              "  {'name': 'Balanced Random Forest',\n",
              "   'macro_f1_mean': np.float64(0.8947143569940257),\n",
              "   'macro_f1_std': np.float64(0.0078602507521117),\n",
              "   'weighted_f1_mean': np.float64(0.973865631253503),\n",
              "   'accuracy_mean': np.float64(0.9715410109266165),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('model',\n",
              "                    BalancedRandomForestClassifier(class_weight='balanced_subsample',\n",
              "                                                   n_estimators=300, n_jobs=-1,\n",
              "                                                   random_state=42))])},\n",
              "  {'name': 'Easy Ensemble',\n",
              "   'macro_f1_mean': np.float64(0.5338336710601936),\n",
              "   'macro_f1_std': np.float64(0.010743588316360702),\n",
              "   'weighted_f1_mean': np.float64(0.8228327011232406),\n",
              "   'accuracy_mean': np.float64(0.7597024962262452),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('model',\n",
              "                    EasyEnsembleClassifier(n_estimators=50, n_jobs=-1,\n",
              "                                           random_state=42))])},\n",
              "  {'name': 'Voting Ensemble',\n",
              "   'macro_f1_mean': np.float64(0.9670180475712286),\n",
              "   'macro_f1_std': np.float64(0.004858035674145086),\n",
              "   'weighted_f1_mean': np.float64(0.9916564311860137),\n",
              "   'accuracy_mean': np.float64(0.9916725741718153),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('model',\n",
              "                    VotingClassifier(estimators=[('rf',\n",
              "                                                  BalancedRandomForestClassifier(random_state=42)),\n",
              "                                                 ('gb',\n",
              "                                                  GradientBoostingClassifier(random_state=42))],\n",
              "                                     voting='soft'))])},\n",
              "  {'name': 'SVM Balanced',\n",
              "   'macro_f1_mean': np.float64(0.94248239383812),\n",
              "   'macro_f1_std': np.float64(0.007093599370181788),\n",
              "   'weighted_f1_mean': np.float64(0.9814816793971138),\n",
              "   'accuracy_mean': np.float64(0.981148896346838),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('model',\n",
              "                    SVC(C=10.0, class_weight='balanced', probability=True,\n",
              "                        random_state=42))])},\n",
              "  {'name': 'SVM + SMOTE',\n",
              "   'macro_f1_mean': np.float64(0.9578192879981822),\n",
              "   'macro_f1_std': np.float64(0.007462707249957514),\n",
              "   'weighted_f1_mean': np.float64(0.9863713748585893),\n",
              "   'accuracy_mean': np.float64(0.9862734564243857),\n",
              "   'pipeline': Pipeline(steps=[('preprocess',\n",
              "                    ColumnTransformer(transformers=[('num',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='median')),\n",
              "                                                                     ('scaler',\n",
              "                                                                      StandardScaler())]),\n",
              "                                                     ['user_id', 'confidence',\n",
              "                                                      'predicted_intensity',\n",
              "                                                      'forecast_length',\n",
              "                                                      'pred_hour', 'pred_dow']),\n",
              "                                                    ('cat',\n",
              "                                                     Pipeline(steps=[('imputer',\n",
              "                                                                      SimpleImputer(strategy='most_frequent')),\n",
              "                                                                     ('onehot',\n",
              "                                                                      OneHotEncoder(drop='first',\n",
              "                                                                                    handle_unknown='ignore'))]),\n",
              "                                                     ['community', 'district',\n",
              "                                                      'indicator',\n",
              "                                                      'pred_date'])])),\n",
              "                   ('sample', SMOTE(k_neighbors=3, random_state=42)),\n",
              "                   ('model', SVC(C=10.0, probability=True, random_state=42))])}])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 5 Advanced Classification Techniques with Automatic Submission Generation ===\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Advanced models\n",
        "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Imbalanced learning\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, EasyEnsembleClassifier\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def setup_data():\n",
        "    \"\"\"Setup function - loads actual data\"\"\"\n",
        "    TRAIN_PATH = \"train.csv\"\n",
        "    TEST_PATH = \"test.csv\"\n",
        "    SAMPLE_SUB_PATH = \"SampleSubmission.csv\"\n",
        "\n",
        "    assert os.path.exists(TRAIN_PATH), f\"Missing: {TRAIN_PATH}\"\n",
        "    assert os.path.exists(TEST_PATH), f\"Missing: {TEST_PATH}\"\n",
        "    assert os.path.exists(SAMPLE_SUB_PATH), f\"Missing: {SAMPLE_SUB_PATH}\"\n",
        "\n",
        "    train = pd.read_csv(TRAIN_PATH)\n",
        "    test = pd.read_csv(TEST_PATH)\n",
        "    sample_sub = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "\n",
        "    # Time parsing function\n",
        "    def parse_time_features(df, time_col='prediction_time'):\n",
        "        df = df.copy()\n",
        "        if time_col in df.columns:\n",
        "            dt = pd.to_datetime(df[time_col].astype(str), dayfirst=True, errors='coerce')\n",
        "            df['pred_hour'] = dt.dt.hour\n",
        "            df['pred_dow'] = dt.dt.dayofweek\n",
        "            df['pred_date'] = dt.dt.date.astype('str')\n",
        "        return df\n",
        "\n",
        "    train = parse_time_features(train)\n",
        "    test = parse_time_features(test)\n",
        "\n",
        "    # Feature selection\n",
        "    TARGET = \"Target\"\n",
        "    ID_COL = \"ID\"\n",
        "    feature_cols = [c for c in train.columns if c not in [TARGET]]\n",
        "    drop_cols = [\"prediction_time\", \"time_observed\", \"indicator_description\"]\n",
        "    feature_cols = [c for c in feature_cols if c not in drop_cols]\n",
        "\n",
        "    # Separate numeric and categorical\n",
        "    numeric_cols = []\n",
        "    cat_cols = []\n",
        "    for c in feature_cols:\n",
        "        if c == ID_COL:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(train[c]):\n",
        "            numeric_cols.append(c)\n",
        "        else:\n",
        "            cat_cols.append(c)\n",
        "\n",
        "    return train, test, sample_sub, feature_cols, numeric_cols, cat_cols, TARGET, ID_COL\n",
        "\n",
        "# Common preprocessing pipeline\n",
        "def get_preprocessor(numeric_cols, cat_cols):\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first'))\n",
        "    ])\n",
        "\n",
        "    return ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, cat_cols)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def conform_to_sample(sample_df: pd.DataFrame, pred_df: pd.DataFrame, id_col: str = \"ID\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a DataFrame that has the exact columns and order of sample_df.\n",
        "    \"\"\"\n",
        "    sample_cols = list(sample_df.columns)\n",
        "    assert id_col in sample_cols, f\"'{id_col}' must be a column in SampleSubmission\"\n",
        "\n",
        "    target_cols = [c for c in sample_cols if c != id_col]\n",
        "    if len(target_cols) == 0:\n",
        "        raise ValueError(\"SampleSubmission must contain at least one target column besides the id.\")\n",
        "\n",
        "    merged = sample_df[[id_col]].merge(pred_df, on=id_col, how=\"left\")\n",
        "\n",
        "    for tcol in target_cols:\n",
        "        if tcol in pred_df.columns:\n",
        "            merged[tcol] = merged[tcol]\n",
        "        else:\n",
        "            pred_only = [c for c in pred_df.columns if c != id_col]\n",
        "            if len(pred_only) == 1:\n",
        "                merged[tcol] = merged[pred_only[0]]\n",
        "            else:\n",
        "                raise ValueError(f\"Cannot map predictions to sample target column '{tcol}'.\")\n",
        "\n",
        "    return merged[sample_cols]\n",
        "\n",
        "def generate_submission(pipeline, X_test, test_ids, sample_sub, technique_name, ID_COL):\n",
        "    \"\"\"Generate and save submission file for a technique\"\"\"\n",
        "    try:\n",
        "        # Make predictions\n",
        "        test_pred = pipeline.predict(X_test)\n",
        "\n",
        "        # Create prediction dataframe\n",
        "        pred_df = pd.DataFrame({ID_COL: test_ids, 'Target': test_pred})\n",
        "\n",
        "        # Conform to sample submission format\n",
        "        submission = conform_to_sample(sample_sub, pred_df, id_col=ID_COL)\n",
        "\n",
        "        # Create safe filename\n",
        "        safe_name = technique_name.replace(' ', '_').replace('+', 'plus').replace('/', '_')\n",
        "        filename = f\"submission_{safe_name}.csv\"\n",
        "\n",
        "        # Save submission\n",
        "        submission.to_csv(filename, index=False)\n",
        "        print(f\"✅ Saved: {filename}\")\n",
        "\n",
        "        return filename\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to generate submission for {technique_name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Evaluation function with submission generation\n",
        "def evaluate_classifier_with_submission(name, pipeline, X_train, y_train, X_test, test_ids, sample_sub, ID_COL, cv_folds=5):\n",
        "    \"\"\"Evaluate classifier and generate submission file\"\"\"\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Evaluating: {name}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "    # Cross-validation scores\n",
        "    macro_f1 = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='f1_macro', n_jobs=-1)\n",
        "    weighted_f1 = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='f1_weighted', n_jobs=-1)\n",
        "    accuracy = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    print(f\"Macro F1:     {macro_f1.mean():.4f} ± {macro_f1.std():.4f}\")\n",
        "    print(f\"Weighted F1:  {weighted_f1.mean():.4f} ± {weighted_f1.std():.4f}\")\n",
        "    print(f\"Accuracy:     {accuracy.mean():.4f} ± {accuracy.std():.4f}\")\n",
        "\n",
        "    # Fit on full training data\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Generate detailed report on training data\n",
        "    y_pred_train = pipeline.predict(X_train)\n",
        "    print(f\"\\nDetailed Report:\")\n",
        "    print(classification_report(y_train, y_pred_train, zero_division=0))\n",
        "\n",
        "    # Generate submission file\n",
        "    submission_file = generate_submission(pipeline, X_test, test_ids, sample_sub, name, ID_COL)\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'macro_f1_mean': macro_f1.mean(),\n",
        "        'macro_f1_std': macro_f1.std(),\n",
        "        'weighted_f1_mean': weighted_f1.mean(),\n",
        "        'accuracy_mean': accuracy.mean(),\n",
        "        'pipeline': pipeline,\n",
        "        'submission_file': submission_file\n",
        "    }\n",
        "\n",
        "# === TECHNIQUE 1: GRADIENT BOOSTING WITH ADVANCED SAMPLING ===\n",
        "def technique_1_gradient_boosting(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL):\n",
        "    \"\"\"Gradient Boosting with multiple sampling strategies\"\"\"\n",
        "\n",
        "    samplers = {\n",
        "        'SMOTE': SMOTE(random_state=42, k_neighbors=3),\n",
        "        'ADASYN': ADASYN(random_state=42, n_neighbors=3),\n",
        "        'SMOTEENN': SMOTEENN(random_state=42, smote=SMOTE(k_neighbors=3)),\n",
        "        'BorderlineSMOTE': BorderlineSMOTE(random_state=42, k_neighbors=3)\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for sampler_name, sampler in samplers.items():\n",
        "        try:\n",
        "            model = GradientBoostingClassifier(\n",
        "                n_estimators=200,\n",
        "                learning_rate=0.1,\n",
        "                max_depth=6,\n",
        "                subsample=0.8,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "            pipeline = ImbPipeline([\n",
        "                ('preprocess', preprocessor),\n",
        "                ('sample', sampler),\n",
        "                ('model', model)\n",
        "            ])\n",
        "\n",
        "            result = evaluate_classifier_with_submission(\n",
        "                f\"GradientBoosting + {sampler_name}\", pipeline,\n",
        "                X_train, y_train, X_test, test_ids, sample_sub, ID_COL\n",
        "            )\n",
        "            results.append(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {sampler_name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 2: LIGHTGBM WITH FOCAL LOSS ===\n",
        "def technique_2_lightgbm_focal(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL):\n",
        "    \"\"\"LightGBM with class balancing\"\"\"\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Standard LightGBM\n",
        "    lgb_standard = lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        bagging_freq=5,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    pipeline_standard = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', lgb_standard)\n",
        "    ])\n",
        "\n",
        "    models.append(('LightGBM Standard', pipeline_standard))\n",
        "\n",
        "    # LightGBM with SMOTE\n",
        "    lgb_smote = lgb.LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.1,\n",
        "        num_leaves=31,\n",
        "        feature_fraction=0.8,\n",
        "        bagging_fraction=0.8,\n",
        "        bagging_freq=5,\n",
        "        random_state=42,\n",
        "        verbosity=-1\n",
        "    )\n",
        "\n",
        "    pipeline_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', lgb_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('LightGBM + SMOTE', pipeline_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier_with_submission(\n",
        "                name, pipeline, X_train, y_train, X_test, test_ids, sample_sub, ID_COL\n",
        "            )\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 3: ENSEMBLE METHODS FOR IMBALANCED DATA ===\n",
        "def technique_3_imbalanced_ensembles(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL):\n",
        "    \"\"\"Specialized ensemble methods for imbalanced datasets\"\"\"\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Balanced Random Forest\n",
        "    brf = BalancedRandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced_subsample'\n",
        "    )\n",
        "\n",
        "    pipeline_brf = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', brf)\n",
        "    ])\n",
        "\n",
        "    models.append(('Balanced Random Forest', pipeline_brf))\n",
        "\n",
        "    # Easy Ensemble\n",
        "    ee = EasyEnsembleClassifier(\n",
        "        n_estimators=50,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    pipeline_ee = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', ee)\n",
        "    ])\n",
        "\n",
        "    models.append(('Easy Ensemble', pipeline_ee))\n",
        "\n",
        "    # Custom Voting Ensemble\n",
        "    rf_balanced = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    gb_weighted = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf_balanced),\n",
        "            ('gb', gb_weighted)\n",
        "        ],\n",
        "        voting='soft'\n",
        "    )\n",
        "\n",
        "    pipeline_voting = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', voting_clf)\n",
        "    ])\n",
        "\n",
        "    models.append(('Voting Ensemble', pipeline_voting))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier_with_submission(\n",
        "                name, pipeline, X_train, y_train, X_test, test_ids, sample_sub, ID_COL\n",
        "            )\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 4: COST-SENSITIVE SVM ===\n",
        "def technique_4_cost_sensitive_svm(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL):\n",
        "    \"\"\"Support Vector Machine with cost-sensitive learning\"\"\"\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # SVM with balanced weights\n",
        "    svm_balanced = SVC(\n",
        "        kernel='rbf',\n",
        "        C=10.0,\n",
        "        gamma='scale',\n",
        "        class_weight='balanced',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_svm = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', svm_balanced)\n",
        "    ])\n",
        "\n",
        "    models.append(('SVM Balanced', pipeline_svm))\n",
        "\n",
        "    # SVM with SMOTE\n",
        "    svm_smote = SVC(\n",
        "        kernel='rbf',\n",
        "        C=10.0,\n",
        "        gamma='scale',\n",
        "        probability=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_svm_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', svm_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('SVM + SMOTE', pipeline_svm_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier_with_submission(\n",
        "                name, pipeline, X_train, y_train, X_test, test_ids, sample_sub, ID_COL\n",
        "            )\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === TECHNIQUE 5: NEURAL NETWORK ===\n",
        "def technique_5_neural_network(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL):\n",
        "    \"\"\"Multi-layer Perceptron with class balancing\"\"\"\n",
        "\n",
        "    models = []\n",
        "\n",
        "    # Standard MLP\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=(200, 100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_mlp = Pipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('model', mlp)\n",
        "    ])\n",
        "\n",
        "    models.append(('Neural Network (MLP)', pipeline_mlp))\n",
        "\n",
        "    # MLP with SMOTE\n",
        "    mlp_smote = MLPClassifier(\n",
        "        hidden_layer_sizes=(200, 100, 50),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        alpha=0.001,\n",
        "        learning_rate_init=0.01,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    pipeline_mlp_smote = ImbPipeline([\n",
        "        ('preprocess', preprocessor),\n",
        "        ('sample', SMOTE(random_state=42, k_neighbors=3)),\n",
        "        ('model', mlp_smote)\n",
        "    ])\n",
        "\n",
        "    models.append(('Neural Network + SMOTE', pipeline_mlp_smote))\n",
        "\n",
        "    results = []\n",
        "    for name, pipeline in models:\n",
        "        try:\n",
        "            result = evaluate_classifier_with_submission(\n",
        "                name, pipeline, X_train, y_train, X_test, test_ids, sample_sub, ID_COL\n",
        "            )\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed {name}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# === MAIN EXECUTION FUNCTION ===\n",
        "def run_all_techniques_with_submissions():\n",
        "    \"\"\"Run all 5 classification techniques, compare results, and generate submissions\"\"\"\n",
        "\n",
        "    print(\"Loading and preparing data...\")\n",
        "    try:\n",
        "        train, test, sample_sub, feature_cols, numeric_cols, cat_cols, TARGET, ID_COL = setup_data()\n",
        "\n",
        "        X_train = train[feature_cols].drop(columns=[ID_COL], errors='ignore')\n",
        "        y_train = train[TARGET]\n",
        "        X_test = test[feature_cols].drop(columns=[ID_COL], errors='ignore')\n",
        "        test_ids = test[ID_COL]\n",
        "\n",
        "        print(f\"Data shape: {X_train.shape}\")\n",
        "        print(f\"Class distribution:\")\n",
        "        print(y_train.value_counts(normalize=True).mul(100).round(2))\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Data files not found: {e}\")\n",
        "        return\n",
        "\n",
        "    # Get preprocessor\n",
        "    preprocessor = get_preprocessor(numeric_cols, cat_cols)\n",
        "\n",
        "    # Store all results\n",
        "    all_results = []\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"RUNNING ALL 5 ADVANCED CLASSIFICATION TECHNIQUES\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Run all techniques\n",
        "    try:\n",
        "        results_1 = technique_1_gradient_boosting(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL)\n",
        "        all_results.extend(results_1)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 1 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_2 = technique_2_lightgbm_focal(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL)\n",
        "        all_results.extend(results_2)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 2 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_3 = technique_3_imbalanced_ensembles(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL)\n",
        "        all_results.extend(results_3)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 3 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_4 = technique_4_cost_sensitive_svm(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL)\n",
        "        all_results.extend(results_4)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 4 failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        results_5 = technique_5_neural_network(preprocessor, X_train, y_train, X_test, test_ids, sample_sub, ID_COL)\n",
        "        all_results.extend(results_5)\n",
        "    except Exception as e:\n",
        "        print(f\"Technique 5 failed: {e}\")\n",
        "\n",
        "    # Summary of results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if all_results:\n",
        "        results_df = pd.DataFrame([\n",
        "            {\n",
        "                'Technique': r['name'],\n",
        "                'Macro F1': f\"{r['macro_f1_mean']:.4f} ± {r['macro_f1_std']:.4f}\",\n",
        "                'Weighted F1': f\"{r['weighted_f1_mean']:.4f}\",\n",
        "                'Accuracy': f\"{r['accuracy_mean']:.4f}\",\n",
        "                'Submission File': r['submission_file'] if r['submission_file'] else 'Failed'\n",
        "            }\n",
        "            for r in all_results\n",
        "        ])\n",
        "\n",
        "        # Sort by Macro F1\n",
        "        results_df['Macro_F1_Score'] = [r['macro_f1_mean'] for r in all_results]\n",
        "        results_df = results_df.sort_values('Macro_F1_Score', ascending=False)\n",
        "        results_df = results_df.drop('Macro_F1_Score', axis=1)\n",
        "\n",
        "        print(results_df.to_string(index=False))\n",
        "\n",
        "        # Best model\n",
        "        best_result = max(all_results, key=lambda x: x['macro_f1_mean'])\n",
        "        print(f\"\\n🏆 BEST PERFORMING MODEL: {best_result['name']}\")\n",
        "        print(f\"   Macro F1: {best_result['macro_f1_mean']:.4f}\")\n",
        "        print(f\"   Submission: {best_result['submission_file']}\")\n",
        "\n",
        "        # Summary of generated files\n",
        "        successful_submissions = [r for r in all_results if r['submission_file']]\n",
        "        print(f\"\\n📁 GENERATED SUBMISSIONS: {len(successful_submissions)} files\")\n",
        "        for result in successful_submissions:\n",
        "            print(f\"   - {result['submission_file']}\")\n",
        "\n",
        "        return best_result['pipeline'], all_results\n",
        "    else:\n",
        "        print(\"No results available. Check data loading and dependencies.\")\n",
        "        return None, []\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the complete analysis with submission generation\n",
        "    best_model, all_results = run_all_techniques_with_submissions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVs5bWiQ1xtK",
        "outputId": "d9001424-c027-487c-9ffc-02ee02501fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preparing data...\n",
            "Data shape: (10928, 10)\n",
            "Class distribution:\n",
            "Target\n",
            "NORAIN        87.96\n",
            "MEDIUMRAIN     6.96\n",
            "HEAVYRAIN      2.88\n",
            "SMALLRAIN      2.20\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "======================================================================\n",
            "RUNNING ALL 5 ADVANCED CLASSIFICATION TECHNIQUES\n",
            "======================================================================\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9818 ± 0.0033\n",
            "Weighted F1:  0.9956 ± 0.0008\n",
            "Accuracy:     0.9956 ± 0.0008\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_GradientBoosting_plus_SMOTE.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + ADASYN\n",
            "==================================================\n",
            "Macro F1:     0.9779 ± 0.0042\n",
            "Weighted F1:  0.9946 ± 0.0010\n",
            "Accuracy:     0.9946 ± 0.0010\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_GradientBoosting_plus_ADASYN.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + SMOTEENN\n",
            "==================================================\n",
            "Macro F1:     0.9536 ± 0.0063\n",
            "Weighted F1:  0.9881 ± 0.0018\n",
            "Accuracy:     0.9882 ± 0.0018\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.98      1.00      0.99       315\n",
            "  MEDIUMRAIN       0.96      1.00      0.98       761\n",
            "      NORAIN       1.00      0.99      1.00      9612\n",
            "   SMALLRAIN       0.89      1.00      0.94       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.96      1.00      0.98     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "✅ Saved: submission_GradientBoosting_plus_SMOTEENN.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: GradientBoosting + BorderlineSMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9802 ± 0.0057\n",
            "Weighted F1:  0.9952 ± 0.0011\n",
            "Accuracy:     0.9951 ± 0.0011\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_GradientBoosting_plus_BorderlineSMOTE.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: LightGBM Standard\n",
            "==================================================\n",
            "Macro F1:     0.9803 ± 0.0031\n",
            "Weighted F1:  0.9954 ± 0.0006\n",
            "Accuracy:     0.9954 ± 0.0006\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_LightGBM_Standard.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: LightGBM + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9818 ± 0.0023\n",
            "Weighted F1:  0.9956 ± 0.0010\n",
            "Accuracy:     0.9956 ± 0.0010\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       1.00      1.00      1.00       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       1.00      1.00      1.00     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_LightGBM_plus_SMOTE.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: Balanced Random Forest\n",
            "==================================================\n",
            "Macro F1:     0.8947 ± 0.0079\n",
            "Weighted F1:  0.9739 ± 0.0027\n",
            "Accuracy:     0.9715 ± 0.0031\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.94      1.00      0.97       315\n",
            "  MEDIUMRAIN       0.94      1.00      0.97       761\n",
            "      NORAIN       1.00      0.98      0.99      9612\n",
            "   SMALLRAIN       0.65      1.00      0.79       240\n",
            "\n",
            "    accuracy                           0.98     10928\n",
            "   macro avg       0.88      0.99      0.93     10928\n",
            "weighted avg       0.99      0.98      0.98     10928\n",
            "\n",
            "✅ Saved: submission_Balanced_Random_Forest.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: Easy Ensemble\n",
            "==================================================\n",
            "Macro F1:     0.5338 ± 0.0107\n",
            "Weighted F1:  0.8228 ± 0.0192\n",
            "Accuracy:     0.7597 ± 0.0300\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.40      0.92      0.55       315\n",
            "  MEDIUMRAIN       0.67      0.52      0.58       761\n",
            "      NORAIN       0.98      0.81      0.89      9612\n",
            "   SMALLRAIN       0.09      0.62      0.15       240\n",
            "\n",
            "    accuracy                           0.78     10928\n",
            "   macro avg       0.53      0.72      0.54     10928\n",
            "weighted avg       0.92      0.78      0.84     10928\n",
            "\n",
            "✅ Saved: submission_Easy_Ensemble.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: Voting Ensemble\n",
            "==================================================\n",
            "Macro F1:     0.9670 ± 0.0049\n",
            "Weighted F1:  0.9917 ± 0.0010\n",
            "Accuracy:     0.9917 ± 0.0011\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.99      0.99      0.99       315\n",
            "  MEDIUMRAIN       0.99      0.97      0.98       761\n",
            "      NORAIN       1.00      1.00      1.00      9612\n",
            "   SMALLRAIN       0.97      0.99      0.98       240\n",
            "\n",
            "    accuracy                           1.00     10928\n",
            "   macro avg       0.99      0.99      0.99     10928\n",
            "weighted avg       1.00      1.00      1.00     10928\n",
            "\n",
            "✅ Saved: submission_Voting_Ensemble.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: SVM Balanced\n",
            "==================================================\n",
            "Macro F1:     0.9425 ± 0.0071\n",
            "Weighted F1:  0.9815 ± 0.0023\n",
            "Accuracy:     0.9811 ± 0.0024\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       0.98      1.00      0.99       315\n",
            "  MEDIUMRAIN       0.90      1.00      0.95       761\n",
            "      NORAIN       1.00      0.99      0.99      9612\n",
            "   SMALLRAIN       0.93      1.00      0.97       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.95      1.00      0.97     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "✅ Saved: submission_SVM_Balanced.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: SVM + SMOTE\n",
            "==================================================\n",
            "Macro F1:     0.9578 ± 0.0075\n",
            "Weighted F1:  0.9864 ± 0.0026\n",
            "Accuracy:     0.9863 ± 0.0027\n",
            "\n",
            "Detailed Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   HEAVYRAIN       1.00      1.00      1.00       315\n",
            "  MEDIUMRAIN       0.93      1.00      0.96       761\n",
            "      NORAIN       1.00      0.99      1.00      9612\n",
            "   SMALLRAIN       1.00      1.00      1.00       240\n",
            "\n",
            "    accuracy                           0.99     10928\n",
            "   macro avg       0.98      1.00      0.99     10928\n",
            "weighted avg       0.99      0.99      0.99     10928\n",
            "\n",
            "✅ Saved: submission_SVM_plus_SMOTE.csv\n",
            "\n",
            "==================================================\n",
            "Evaluating: Neural Network (MLP)\n",
            "==================================================\n",
            "Failed Neural Network (MLP): \n",
            "All the 5 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 754, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 476, in _fit\n",
            "    self._fit_stochastic(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 660, in _fit_stochastic\n",
            "    self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 708, in _update_no_improvement_count\n",
            "    val_score = self._score(X_val, y_val)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1194, in _score\n",
            "    return super()._score_with_function(X, y, score_function=accuracy_score)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 769, in _score_with_function\n",
            "    if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
            "\n",
            "\n",
            "==================================================\n",
            "Evaluating: Neural Network + SMOTE\n",
            "==================================================\n",
            "Failed Neural Network + SMOTE: \n",
            "All the 5 fits failed.\n",
            "It is very likely that your model is misconfigured.\n",
            "You can try to debug the error by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/imblearn/pipeline.py\", line 526, in fit\n",
            "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 754, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 476, in _fit\n",
            "    self._fit_stochastic(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 660, in _fit_stochastic\n",
            "    self._update_no_improvement_count(early_stopping, X_val, y_val)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 708, in _update_no_improvement_count\n",
            "    val_score = self._score(X_val, y_val)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1194, in _score\n",
            "    return super()._score_with_function(X, y, score_function=accuracy_score)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 769, in _score_with_function\n",
            "    if np.isnan(y_pred).any() or np.isinf(y_pred).any():\n",
            "       ^^^^^^^^^^^^^^^^\n",
            "TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
            "\n",
            "\n",
            "======================================================================\n",
            "FINAL RESULTS SUMMARY\n",
            "======================================================================\n",
            "                         Technique        Macro F1 Weighted F1 Accuracy                                      Submission File\n",
            "                  LightGBM + SMOTE 0.9818 ± 0.0023      0.9956   0.9956                   submission_LightGBM_plus_SMOTE.csv\n",
            "          GradientBoosting + SMOTE 0.9818 ± 0.0033      0.9956   0.9956           submission_GradientBoosting_plus_SMOTE.csv\n",
            "                 LightGBM Standard 0.9803 ± 0.0031      0.9954   0.9954                     submission_LightGBM_Standard.csv\n",
            "GradientBoosting + BorderlineSMOTE 0.9802 ± 0.0057      0.9952   0.9951 submission_GradientBoosting_plus_BorderlineSMOTE.csv\n",
            "         GradientBoosting + ADASYN 0.9779 ± 0.0042      0.9946   0.9946          submission_GradientBoosting_plus_ADASYN.csv\n",
            "                   Voting Ensemble 0.9670 ± 0.0049      0.9917   0.9917                       submission_Voting_Ensemble.csv\n",
            "                       SVM + SMOTE 0.9578 ± 0.0075      0.9864   0.9863                        submission_SVM_plus_SMOTE.csv\n",
            "       GradientBoosting + SMOTEENN 0.9536 ± 0.0063      0.9881   0.9882        submission_GradientBoosting_plus_SMOTEENN.csv\n",
            "                      SVM Balanced 0.9425 ± 0.0071      0.9815   0.9811                          submission_SVM_Balanced.csv\n",
            "            Balanced Random Forest 0.8947 ± 0.0079      0.9739   0.9715                submission_Balanced_Random_Forest.csv\n",
            "                     Easy Ensemble 0.5338 ± 0.0107      0.8228   0.7597                         submission_Easy_Ensemble.csv\n",
            "\n",
            "🏆 BEST PERFORMING MODEL: LightGBM + SMOTE\n",
            "   Macro F1: 0.9818\n",
            "   Submission: submission_LightGBM_plus_SMOTE.csv\n",
            "\n",
            "📁 GENERATED SUBMISSIONS: 11 files\n",
            "   - submission_GradientBoosting_plus_SMOTE.csv\n",
            "   - submission_GradientBoosting_plus_ADASYN.csv\n",
            "   - submission_GradientBoosting_plus_SMOTEENN.csv\n",
            "   - submission_GradientBoosting_plus_BorderlineSMOTE.csv\n",
            "   - submission_LightGBM_Standard.csv\n",
            "   - submission_LightGBM_plus_SMOTE.csv\n",
            "   - submission_Balanced_Random_Forest.csv\n",
            "   - submission_Easy_Ensemble.csv\n",
            "   - submission_Voting_Ensemble.csv\n",
            "   - submission_SVM_Balanced.csv\n",
            "   - submission_SVM_plus_SMOTE.csv\n"
          ]
        }
      ]
    }
  ]
}